import platform
import torch
import subprocess
import pkg_resources

print("=" * 60)
print("ğŸ” Python / CUDA / PyTorch ç¯å¢ƒæ£€æµ‹è„šæœ¬")
print("=" * 60)

# --- Python ç¯å¢ƒ ---
print(f"ğŸ Python ç‰ˆæœ¬: {platform.python_version()}")
print(f"ğŸ§  ç³»ç»Ÿå¹³å°: {platform.system()} {platform.release()}")
print()

# --- PyTorch ä¿¡æ¯ ---
print("ğŸ”¥ PyTorch ä¿¡æ¯:")
print(f"  torch ç‰ˆæœ¬: {torch.__version__}")
print(f"  CUDA runtime: {torch.version.cuda}")
print(f"  cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
print(f"  CUDA æ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"  å½“å‰ GPU: {torch.cuda.get_device_name(0)}")
    print(f"  GPU æ•°é‡: {torch.cuda.device_count()}")
print()

# --- é©±åŠ¨ä¿¡æ¯ (é€šè¿‡ nvidia-smi) ---
try:
    smi_output = subprocess.check_output(["nvidia-smi"], text=True)
    print("ğŸ§© nvidia-smi è¾“å‡º:")
    print(smi_output.split("\n")[0])
    for line in smi_output.split("\n"):
        if "Driver Version" in line:
            print(line.strip())
            break
except Exception as e:
    print("âš ï¸ æ— æ³•è¿è¡Œ nvidia-smi:", e)
print()

# --- æ£€æŸ¥ä¸»è¦ CUDA ç›¸å…³åŒ… ---
print("ğŸ“¦ CUDA ç›¸å…³ Python åŒ…:")
cuda_packages = ["torch", "torchvision", "torchaudio", "triton", "xformers", "nvidia-cublas-cu12", "nvidia-cuda-runtime-cu12"]
for dist in pkg_resources.working_set:
    if any(name in dist.project_name.lower() for name in cuda_packages):
        print(f"  {dist.project_name}=={dist.version}")
print("=" * 60)
